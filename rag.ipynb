{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2+rocm5.6\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'elements' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/aurorabuilder/elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.prompts import ChatPromptTemplate, ChatMessagePromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_opencl: selecting platform: 'AMD Accelerated Parallel Processing'\n",
      "ggml_opencl: selecting device: 'gfx1030'\n",
      "ggml_opencl: device FP16 support: true\n",
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from models/neural-chat-7b-v3-3.Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = intel_neural-chat-7b-v3-3\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = intel_neural-chat-7b-v3-3\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size       =    0.11 MiB\n",
      "llm_load_tensors: using OpenCL for GPU acceleration\n",
      "llm_load_tensors: system memory used  =   86.05 MiB\n",
      "llm_load_tensors: VRAM used           = 4807.05 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 32768\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_new_context_with_model: KV self size  = 4096.00 MiB, K (f16): 2048.00 MiB, V (f16): 2048.00 MiB\n",
      "llama_build_graph: non-view tensors processed: 676/676\n",
      "llama_new_context_with_model: compute buffer total size = 36.56 MiB\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "model = LlamaCpp(\n",
    "    model_path=\"models/neural-chat-7b-v3-3.Q5_K_M.gguf\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=512,\n",
    "    top_p=0.95,\n",
    "    # callback_manager=callback_manager,\n",
    "    verbose=True, # Verbose is required to pass to the callback manager\n",
    "    n_gpu_layers=-1,\n",
    "    echo=True,\n",
    "    n_ctx=1024*32,\n",
    "    stop=[\"### System\", \"### User\", \"### Assistant\", \"User:\", \"Player:\", \"DM:\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc95ad078c2146b7bbb0bf6c04fecb58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e03140400acf4187b14af45062e33455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42530688c7684c639d135c3f5c455d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05050dfd552f4442b7ccea1c2da26520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1405eddd6f9540e58027231d37e834bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65946f8775464f91ad15d759450f4fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed1f6f8356b4b4da46913146e119531",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359b2135a6dd41aca791f962a78230c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead53b2e013145819827a7006c876903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aefeb214dc8843b9b2d5c1716e2fab85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0a86d08de4946dab5ac65af1516ea43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fad1e78824f406eb684c50ed6075774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97100659f3594230859211558c7d474e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0041a692c4cc4773a68c07fe76bf455f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "hf = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs=model_kwargs,\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, UnstructuredXMLLoader\n",
    "\n",
    "import os \n",
    "import glob  \n",
    "# Define the directory path \n",
    "directory_path = 'elements/core/players-handbook'  \n",
    "# Initialize a list to store the XML file contents as strings \n",
    "xml_strings = []  \n",
    "# Use the glob module to recursively find XML files in the directory and its subfolders \n",
    "xml_files = glob.glob('*.xml', root_dir=directory_path, recursive=True)\n",
    "\n",
    "documents = []\n",
    "\n",
    "for xml_file in xml_files:\n",
    "    loader = UnstructuredXMLLoader(os.path.join(directory_path, xml_file))\n",
    "    documents.extend(loader.load())\n",
    "# Loop through each XML file and read its content as a string \n",
    "# for xml_file in xml_files:     \n",
    "#     with open(os.path.join(directory_path, xml_file), 'r', encoding='utf-8') as file:         \n",
    "#         xml_string = file.read()         \n",
    "#         xml_strings.append(xml_string)  \n",
    "# Now, the xml_strings list contains the contents of all XML files as strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2192, which is longer than the specified 1024\n",
      "Created a chunk of size 1089, which is longer than the specified 1024\n",
      "Created a chunk of size 1220, which is longer than the specified 1024\n",
      "Created a chunk of size 1150, which is longer than the specified 1024\n",
      "Created a chunk of size 1253, which is longer than the specified 1024\n",
      "Created a chunk of size 1259, which is longer than the specified 1024\n",
      "Created a chunk of size 1256, which is longer than the specified 1024\n",
      "Created a chunk of size 1362, which is longer than the specified 1024\n",
      "Created a chunk of size 1217, which is longer than the specified 1024\n",
      "Created a chunk of size 1233, which is longer than the specified 1024\n",
      "Created a chunk of size 1173, which is longer than the specified 1024\n",
      "Created a chunk of size 2253, which is longer than the specified 1024\n",
      "Created a chunk of size 1428, which is longer than the specified 1024\n",
      "Created a chunk of size 1245, which is longer than the specified 1024\n",
      "Created a chunk of size 1123, which is longer than the specified 1024\n",
      "Created a chunk of size 1067, which is longer than the specified 1024\n",
      "Created a chunk of size 1076, which is longer than the specified 1024\n",
      "Created a chunk of size 1161, which is longer than the specified 1024\n",
      "Created a chunk of size 1241, which is longer than the specified 1024\n",
      "Created a chunk of size 1245, which is longer than the specified 1024\n",
      "Created a chunk of size 1809, which is longer than the specified 1024\n",
      "Created a chunk of size 2228, which is longer than the specified 1024\n",
      "Created a chunk of size 1210, which is longer than the specified 1024\n",
      "Created a chunk of size 1429, which is longer than the specified 1024\n",
      "Created a chunk of size 1185, which is longer than the specified 1024\n",
      "Created a chunk of size 1280, which is longer than the specified 1024\n",
      "Created a chunk of size 1319, which is longer than the specified 1024\n",
      "Created a chunk of size 2338, which is longer than the specified 1024\n",
      "Created a chunk of size 1092, which is longer than the specified 1024\n",
      "Created a chunk of size 1238, which is longer than the specified 1024\n",
      "Created a chunk of size 1236, which is longer than the specified 1024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Document(page_content='Companions\\nCompanion creatures from the Player’s Handbook.\\nWizards of the Coast\\n16\\n14\\n14\\n6\\n12\\n7\\n12\\n19 (3d8+6)\\n30 ft., climb 30 ft.\\n—\\nAthletics +5, Perception +3\\nBeast\\nMedium\\nunaligned\\n1/2\\nID_WOTC_PHB_COMPANION_ACTION_APE_MULTIATTACK,ID_WOTC_PHB_COMPANION_ACTION_APE_FIST,ID_WOTC_PHB_COMPANION_ACTION_APE_ROCK\\nThe ape makes two fist attacks.\\nThe ape makes two fist attacks.\\nMelee Weapon Attack: +5 to hit, reach 5 ft., one target. Hit: 6 (1d6 + 3) bludgeoning damage.\\nMelee Weapon Attack: +5 to hit, reach 5 ft., one target. Hit: 6 (1d6 + 3) bludgeoning damage.\\nRanged Weapon Attack: +5 to hit, range 25/50 ft., one target. Hit: 6 (1d6 + 3) bludgeoning damage.\\nRanged Weapon Attack: +5 to hit, range 25/50 ft., one target. Hit: 6 (1d6 + 3) bludgeoning damage.\\n8\\n14\\n11\\n4\\n12\\n6\\n12\\n3 (1d6)\\n30 ft., climb 30 ft.\\n—\\nBeast\\nSmall\\nunaligned\\n0\\nID_WOTC_PHB_COMPANION_TRAIT_BABOON_PACK_TACTICS\\nID_WOTC_PHB_COMPANION_ACTION_BABOON_BITE', metadata={'source': 'elements/core/players-handbook/companions.xml'})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1024, chunk_overlap=30, separator=\"\\n\")\n",
    "docs = text_splitter.split_documents(documents=documents)\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for CohereRerank\n__root__\n  Did not find cohere_api_key, please add an environment variable `COHERE_API_KEY` which contains it, or pass `cohere_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrievers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ContextualCompressionRetriever\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mretrievers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_compressors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CohereRerank\n\u001b[0;32m----> 8\u001b[0m compressor \u001b[38;5;241m=\u001b[39m \u001b[43mCohereRerank\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m compression_retriever \u001b[38;5;241m=\u001b[39m ContextualCompressionRetriever(\n\u001b[1;32m     10\u001b[0m     base_compressor\u001b[38;5;241m=\u001b[39mcompressor, base_retriever\u001b[38;5;241m=\u001b[39mretriever\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for CohereRerank\n__root__\n  Did not find cohere_api_key, please add an environment variable `COHERE_API_KEY` which contains it, or pass `cohere_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "document_texts = [doc.page_content for doc in docs]\n",
    "vectorstore = FAISS.from_texts(texts=document_texts, embedding=hf)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20})\n",
    "\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CohereRerank\n",
    "\n",
    "# TODO cohere is a paid API. Need to figure out how to replace it with an OS rerank model like https://huggingface.co/BAAI/bge-reranker-large\n",
    "# TODO probably we can extend the BaseDocumentCompressor to use a reranker HF model\n",
    "compressor = CohereRerank()\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"### System:\n",
    "Answer the user's question based only on the following context:\n",
    "{context}\n",
    "\n",
    "### User: Question: {question}\n",
    "\n",
    "### Assistant:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the attack bonus of a LVL 2 Barbarian?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<context,question>] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the attack bonus of a LVL 2 Barbarian?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<context,question> > 4:chain:RunnablePassthrough] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"What is the attack bonus of a LVL 2 Barbarian?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<context,question> > 4:chain:RunnablePassthrough] [0ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"What is the attack bonus of a LVL 2 Barbarian?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<context,question>] [9ms] Exiting Chain run with output:\n",
      "\u001b[0m[outputs]\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 5:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 5:prompt:ChatPromptTemplate] [0ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"chat\",\n",
      "    \"ChatPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"### System:\\nAnswer the user's question based only on the following context:\\n[Document(page_content='When you take the Attack action and attack with only a glaive, halberd, quarterstaff, or spear, you can use a bonus action to make a melee attack with the opposite end of the weapon. The weapon’s damage die for this attack is a d4, and the attack deals bludgeoning damage. This attack uses the same ability modifier as the primary attack.\\\\n\\\\t\\\\t\\\\tWhile you are wielding a glaive, halberd, pike, quarterstaff, or spear, other creatures provoke an opportunity attack from you when they enter your reach.\\\\nChoose one ability score. You gain the following benefits:\\\\nIncrease the chosen ability score by 1, to a maximum of 20.\\\\nYou gain proficiency in saving throws using the chosen ability.\\\\nResilient\\\\nYour Strength increases by 1.\\\\nYou gain proficiency with Strength saving throws.\\\\nResilient\\\\nYour Dexterity increases by 1.\\\\nYou gain proficiency with Dexterity saving throws.\\\\nResilient\\\\nYour Constitution increases by 1.\\\\nYou gain proficiency with Constitution saving throws.\\\\nResilient\\\\nYour Intelligence increases by 1.'), Document(page_content=\\\"Melee Weapon Attack: +5 to hit, reach 5 ft., one creature. Hit: 1 piercing damage.\\\\n12\\\\n15\\\\n12\\\\n3\\\\n12\\\\n6\\\\n13 (natural armor)\\\\n11 (2d8+2)\\\\n40 ft.\\\\n—\\\\nPerception +3, Stealth +4\\\\nBeast\\\\nMedium\\\\nunaligned\\\\n1/4\\\\nID_WOTC_PHB_COMPANION_TRAIT_WOLF_KEEN_HEARING_AND_SMELL,ID_WOTC_PHB_COMPANION_TRAIT_WOLF_PACK_TACTICS\\\\nID_WOTC_PHB_COMPANION_ACTION_WOLF_BITE\\\\nThe wolf has advantage on Wisdom (Perception) checks that rely on hearing or smell.\\\\nThe wolf has advantage on Wisdom (Perception) checks that rely on hearing or smell.\\\\nThe wolf has advantage on an attack roll against a creature if at least one of the wolf's allies is within 5 ft. of the creature and the ally isn't incapacitated.\\\\nThe wolf has advantage on an attack roll against a creature if at least one of the wolf's allies is within 5 ft. of the creature and the ally isn't incapacitated.\\\\nMelee Weapon Attack: +4 to hit, reach 5 ft., one target. Hit: 7 (2d4 + 2) piercing damage. If the target is a creature, it must succeed on a DC 11 Strength saving throw or be knocked prone.\\\"), Document(page_content='At Higher Levels.\\\\n4\\\\nEnchantment\\\\n1 action\\\\nConcentration, up to 1 minute\\\\n60 feet\\\\ntrue\\\\ntrue\\\\nfalse\\\\ntrue\\\\nfalse\\\\nBard, Sorcerer, Warlock, Wizard'), Document(page_content='At Higher Levels.\\\\n8\\\\nEnchantment\\\\n1 action\\\\nConcentration, up to 1 hour\\\\n60 feet\\\\ntrue\\\\ntrue\\\\nfalse\\\\ntrue\\\\nfalse\\\\nBard, Sorcerer, Wizard')]\\n\\n### User: Question: What is the attack bonus of a LVL 2 Barbarian?\\n\\n### Assistant:\\n\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 6:llm:LlamaCpp] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"Human: ### System:\\nAnswer the user's question based only on the following context:\\n[Document(page_content='When you take the Attack action and attack with only a glaive, halberd, quarterstaff, or spear, you can use a bonus action to make a melee attack with the opposite end of the weapon. The weapon’s damage die for this attack is a d4, and the attack deals bludgeoning damage. This attack uses the same ability modifier as the primary attack.\\\\n\\\\t\\\\t\\\\tWhile you are wielding a glaive, halberd, pike, quarterstaff, or spear, other creatures provoke an opportunity attack from you when they enter your reach.\\\\nChoose one ability score. You gain the following benefits:\\\\nIncrease the chosen ability score by 1, to a maximum of 20.\\\\nYou gain proficiency in saving throws using the chosen ability.\\\\nResilient\\\\nYour Strength increases by 1.\\\\nYou gain proficiency with Strength saving throws.\\\\nResilient\\\\nYour Dexterity increases by 1.\\\\nYou gain proficiency with Dexterity saving throws.\\\\nResilient\\\\nYour Constitution increases by 1.\\\\nYou gain proficiency with Constitution saving throws.\\\\nResilient\\\\nYour Intelligence increases by 1.'), Document(page_content=\\\"Melee Weapon Attack: +5 to hit, reach 5 ft., one creature. Hit: 1 piercing damage.\\\\n12\\\\n15\\\\n12\\\\n3\\\\n12\\\\n6\\\\n13 (natural armor)\\\\n11 (2d8+2)\\\\n40 ft.\\\\n—\\\\nPerception +3, Stealth +4\\\\nBeast\\\\nMedium\\\\nunaligned\\\\n1/4\\\\nID_WOTC_PHB_COMPANION_TRAIT_WOLF_KEEN_HEARING_AND_SMELL,ID_WOTC_PHB_COMPANION_TRAIT_WOLF_PACK_TACTICS\\\\nID_WOTC_PHB_COMPANION_ACTION_WOLF_BITE\\\\nThe wolf has advantage on Wisdom (Perception) checks that rely on hearing or smell.\\\\nThe wolf has advantage on Wisdom (Perception) checks that rely on hearing or smell.\\\\nThe wolf has advantage on an attack roll against a creature if at least one of the wolf's allies is within 5 ft. of the creature and the ally isn't incapacitated.\\\\nThe wolf has advantage on an attack roll against a creature if at least one of the wolf's allies is within 5 ft. of the creature and the ally isn't incapacitated.\\\\nMelee Weapon Attack: +4 to hit, reach 5 ft., one target. Hit: 7 (2d4 + 2) piercing damage. If the target is a creature, it must succeed on a DC 11 Strength saving throw or be knocked prone.\\\"), Document(page_content='At Higher Levels.\\\\n4\\\\nEnchantment\\\\n1 action\\\\nConcentration, up to 1 minute\\\\n60 feet\\\\ntrue\\\\ntrue\\\\nfalse\\\\ntrue\\\\nfalse\\\\nBard, Sorcerer, Warlock, Wizard'), Document(page_content='At Higher Levels.\\\\n8\\\\nEnchantment\\\\n1 action\\\\nConcentration, up to 1 hour\\\\n60 feet\\\\ntrue\\\\ntrue\\\\nfalse\\\\ntrue\\\\nfalse\\\\nBard, Sorcerer, Wizard')]\\n\\n### User: Question: What is the attack bonus of a LVL 2 Barbarian?\\n\\n### Assistant:\"\n",
      "  ]\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 6:llm:LlamaCpp] [53.51s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \" The given context does not specifically mention any details about a Level 2 Barbarian's attack bonus.\",\n",
      "        \"generation_info\": null,\n",
      "        \"type\": \"Generation\"\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": null,\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 7:parser:StrOutputParser] Entering Parser run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \" The given context does not specifically mention any details about a Level 2 Barbarian's attack bonus.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 7:parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \" The given context does not specifically mention any details about a Level 2 Barbarian's attack bonus.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [53.52s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \" The given context does not specifically mention any details about a Level 2 Barbarian's attack bonus.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     971.57 ms\n",
      "llama_print_timings:      sample time =       2.87 ms /    23 runs   (    0.12 ms per token,  8027.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   52303.31 ms /   816 tokens (   64.10 ms per token,    15.60 tokens per second)\n",
      "llama_print_timings:        eval time =    1038.48 ms /    22 runs   (   47.20 ms per token,    21.18 tokens per second)\n",
      "llama_print_timings:       total time =   53505.86 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" The given context does not specifically mention any details about a Level 2 Barbarian's attack bonus.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.globals import set_verbose, set_debug\n",
    "set_debug(True)\n",
    "chain.invoke(\"What is the attack bonus of a LVL 2 Barbarian?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.2+rocm5.6'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import torch;print(torch.cuda.is_available());\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
